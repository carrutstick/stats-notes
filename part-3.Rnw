\documentclass{beamer}

\usepackage{amsmath}

\author{Isaac Carruthers}
\title{Statistics, As I Understand Them}
\subtitle{Part 3: Conjugate Priors and Interpretation of Distributions}

\setkeys{Gin}{width=0.9\textwidth,height=0.7\textheight}

\begin{document}
\frame{\titlepage}

\frame{\frametitle{Quick Review}
\centering
We often want to estimate some parameter $\lambda$ by combining our prior expectations
$p(\lambda)$, observations $\vec x$, and model of how observations are generated
$p(\lambda|\vec x)$.
\vspace{12pt}\pause

We do this with \textbf{Bayesian Inference:}
\[p(\lambda|\vec x) = \frac{P(\vec x|\lambda)P(\lambda)}{
    \int_\lambda P(\vec x|\lambda) P(\lambda) d\lambda}\]\pause

Note that once we have $p(\lambda|\vec x)$, we can treat it \emph{exactly how we treat a prior}.
In particular, we can observe new evidence $\vec y$, and use the above formula to get $p(\lambda
|\vec x, \vec y)$.\vspace{12pt}

This is a system for \textbf{Belief Updating}.
}


\frame{\frametitle{Quick Review}
\centering
<<poissinfer,fig=TRUE,echo=FALSE>>=
right <- 2.5
dx <- .01
x <- seq(from=-.1, to=right, by=dx)
p <- dpois(x=1, lambda=x)
plot(xlim=c(-.1, right), ylim=c(0,2), x=NULL, y=NULL,
     xlab="Lambda", ylab="Probability Density", bty='n', las=1)
segments(c(-.1, 0, 1), c(0, 1, 0), c(0, 1, right), c(0, 1, 0), col='black')
points(c(0, 0, 1, 1), c(0, 1, 1, 0), pch=21, col='black', bg=c('white','black','black','white'))
@
}


\frame{\frametitle{Quick Review}
\centering
<<poissinfer2,fig=TRUE,echo=FALSE>>=
<<poissinfer>>
lines(x, p, col='red')
@
}


\frame{\frametitle{Quick Review}
\centering
<<poissinfer3,fig=TRUE,echo=FALSE>>=
<<poissinfer>>
lines(x, p, col='red')
p[x > 1] <- 0
p[x < 0] <- 0
p <- p / sum(p) / dx
lines(x, p, col='darkgreen')
@
}


\frame{\frametitle{Inference with Gaussians}
Reminder:
\[p(x;\mu,\sigma) = \frac1{\sqrt{2\pi}\sigma} e^{-\left(\frac{x-\mu}{\sqrt2\sigma}\right)^2}\]
\[p(x;\mu,\sigma) \propto e^{-\left(\frac{x-\mu}{\sqrt2\sigma}\right)^2}\]
}


\frame{\frametitle{Inference with Gaussians}
\centering
<<gaussinfer,fig=TRUE,echo=FALSE>>=
dx <- .01
x <- seq(from=0, to=10, by=dx)
p <- dnorm(x, mean=3, sd=2)
l <- dnorm(x, mean=6, sd=1.5)
plot(x, p, type='l', col='black', ylim=c(0, .5), bty='n', las=1)
lines(x, l, col='red')
abline(v=3, col='black', lty=2)
abline(v=6, col='red', lty=2)
@
}


\frame{\frametitle{Inference with Gaussians}
\centering
<<gaussinfer2,fig=TRUE,echo=FALSE>>=
<<gaussinfer>>
pp <- p * l
pp <- pp / sum(pp) / dx
lines(x, pp, col='darkgreen')
mu <- sum(pp * x) / sum(pp)
abline(v=mu, col='darkgreen', lty=2)
@
}


\frame{\frametitle{Inference with Gaussians}
\begin{align*}
	\mathcal{N}(\mu_1,\sigma_1)*\mathcal{N}(\mu_2,\sigma_2) &\propto
	e^{-\left(\frac{x-\mu_1}{\sqrt2\sigma_1}\right)^2}
	e^{-\left(\frac{x-\mu_2}{\sqrt2\sigma_2}\right)^2}\\
\end{align*}
\begin{align*}
	&= \exp\left[-\left(\frac{x-\mu_1}{\sqrt2\sigma_1}\right)^2
    -\left(\frac{x-\mu_2}{\sqrt2\sigma_2}\right)^2\right] \\
    &\propto \exp\left[-\frac{1}{2\sigma_1^2\sigma_2^2}
	\left(\sigma_2^2(x-\mu_1)^2+\sigma_1^2(x-\mu_2)^2\right)\right]\\
    &= \exp\left[-\frac1{2\sigma_1^2\sigma_2^2}\left(
	(\sigma_1^2+\sigma_2^2)x^2-2(\sigma_2^2\mu_1+\sigma_1^2\mu_2)x
    +\sigma_2^2\mu_1^2+\sigma_1^2\mu_2^2\right)\right]
\end{align*}
}


\frame{\frametitle{Algebraic Digression}
\begin{align*}
	ax^2+bx+c&=ax^2+bx+c+\frac{b^2}{4a}-\frac{b^2}{4a}\\
	&=\left(\sqrt{a}x+\frac{b}{2\sqrt{a}}\right)^2 + c -\frac{b^2}{4a}
\end{align*}\pause
\begin{align*}
	e^{ax^2+bx+c}&=e^{\left(\sqrt{a}x+\frac{b}{2\sqrt{a}}\right)^2 + c -\frac{b^2}{4a}}\\
	&=\left(e^{c-\frac{b^2}{4a}}\right)
	\left(e^{\left(\sqrt{a}x+\frac{b}{2\sqrt{a}}\right)^2}\right)
\end{align*}
}


\frame{\frametitle{Inference with Gaussians}
\begin{align*}
	a&\equiv \frac{\sigma_1^2+\sigma_2^2}{2\sigma_1^2\sigma_2^2}
	& b&\equiv -\frac{\sigma_2^2\mu_1+\sigma_1^2\mu_2}{\sigma_1^2\sigma_2^2}
\end{align*}\pause
\begin{align*}
	\mathcal{N}(\mu_1,\sigma_1)*\mathcal{N}(\mu_2,\sigma_2) &\propto
	%\exp\left[-\left(\sqrt{\frac{\sigma_1^2+\sigma_2^2}{2\sigma_1^2\sigma_2^2}}x-
	%\frac{\sigma_2^2\mu_1+\sigma_1^2\mu_2}{\sqrt{\sigma_1^2+\sigma_2^2}}\right)^2\right]
	\mathcal{N}\left(\frac{\mu_1\sigma_2^2+\mu_2\sigma_1^2}{\sigma_2^2+\sigma_1^2},
	\frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}\right)
\end{align*}
}


\frame{\frametitle{Conjugate Priors}
\centering
This means that if we have a gaussian prior, and our evidence has a gaussian likelihood,
then our posterior distribution is also gaussian.\pause\vspace{12pt}

That means we can keep adding evidence to our model without making its form any more complicated.
\pause\vspace{12pt}

Another way to say this is that the gaussian distribution is its own \textbf{Conjugate Prior}.
}


\frame{\frametitle{Digression: The Beta Distribution}
The \textbf{Beta Distribution} $\mathcal{B}(\alpha,\beta)$ is defined on $[0,1]$, and is often used
to model uncertainty in a probability.
\vspace{12pt}

Its pdf is:
\[p(q;\alpha,\beta) = \frac{q^{\alpha-1}(1-q)^{\beta-1}}{\mathbf{B}(\alpha,\beta)}\]

Where $\mathbf{B}$ is the \textbf{Beta Function}:
\[\mathbf{B}(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\]

And $\Gamma$ is the \textbf{Gamma Function}; an extension of the factorial to the real numbers.
In particular, if $i$ is a positive integer, then
\[\Gamma(i) = (i-1)!\]
}


\frame{\frametitle{Digression: The Beta Distribution}
\centering
<<beta,fig=TRUE,echo=FALSE>>=
x <- seq(from=0, to=1, by=.01)
y1 <- dbeta(x, 1, 1)
y2 <- dbeta(x, 10, 5)
plot(x, y2, xlab='q', ylab='p(q)', bty='n', type='l', col='blue')
lines(x, y1, col='red')
legend(.05, 2.5, c(expression(paste('(',alpha,'=1, ',beta,'=1)')),
                   expression(paste('(',alpha,'=10, ',beta,'=5)'))),
       lty=c(1,1), col=c('red', 'blue'))
@
}


\frame{\frametitle{Back to the Binomial Distribution}
\centering
Suppose I have a biased coin with some probability $q$ of landing on heads, but you don't know $q$.
\vspace{12pt}\pause

You see me flip the coin $n$ times, of which $k$ land on heads. What do you now believe about $q$?
\vspace{12pt}\pause

As we've seen before, the likelihood of $k$ given $q$ is
\[p(k;n,q)={n\choose k}q^k(1-q)^{n-k}\]\pause

If we have a beta distribution $\mathcal{B}(\alpha,\beta)$ as a prior, then the posterior after
observing $k$ heads in $n$ flips is $\mathcal{B}(\alpha+k,\beta+n-k)$, which is just another beta
distribution! \vspace{12pt}

You guessed it: the beta distribution is the \textbf{Conjugate Prior} of the binomial
distribution!
}


\frame{\frametitle{Back to the Binomial Distribution}
\centering
In this case, the parameters of our prior have a very simple interpretation:
\vspace{12pt}

Saying ``my prior is $\mathcal{B}(\alpha,\beta)$'' roughly translates to ``I'm going to pretend
that I've already seen $\alpha+\beta$ flips, $\alpha$ of which were heads.''
}


\frame{\frametitle{Binomial Example}
<<binom,fig=TRUE,echo=FALSE>>=
x <- seq(from=0, to=1, by=.01)
y1 <- dbeta(x, 10, 10)
y2 <- dbeta(x, 11, 10)
y3 <- dbeta(x, 20, 10)
plot(x, y1, xlab='q', ylab='p(q)', bty='n', type='l', ylim=c(0,5))
lines(x, y2, col='blue')
lines(x, y3, col='purple')
@
}


\frame{\frametitle{Binomial Example}
<<binom2,fig=TRUE,echo=FALSE>>=
x <- seq(from=0, to=1, by=.01)
y1 <- dbeta(x, 100, 100)
y2 <- dbeta(x, 101, 100)
y3 <- dbeta(x, 110, 100)
plot(x, y1, xlab='q', ylab='p(q)', bty='n', type='l')
lines(x, y2, col='blue')
lines(x, y3, col='purple')
@
}


\frame{\frametitle{Flexibility Through Linear Composition}
\centering
Having to shoehorn our beliefs into a particular distribution can be restrictive.
\vspace{12pt}

What if you tell me that your coin is 70\% biased, but you don't tell me if it's biased for heads
or for tails? How do I represent my beliefs, and how do I update them as you flip the coin?
\vspace{12pt}\pause

Fortunately, a linear combination of conjugate prior distributions also keeps its form after
a belief update!
}


\frame{\frametitle{Binomial Example}
<<binom3,fig=TRUE,echo=FALSE>>=
x <- seq(from=0, to=1, by=.001)
y1 <- dbeta(x, 700, 300) + dbeta(x, 300, 700)
y1 <- y1 / sum(y1)
y2 <- y1 * dbinom(7, size=10, prob=x)
y2 <- y2 / sum(y2)
#y3 <- dbeta(x, 110, 100)
plot(x, y1, xlab='q', ylab='p(q)', bty='n', type='l', ylim=c(0, .03))
lines(x, y2, col='blue')
#lines(x, y3, col='purple')
@
}


\frame{\frametitle{Homework}
Come up with some examples for us to think about!
}

\end{document}
