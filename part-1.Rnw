\documentclass{beamer}

\usepackage{amsmath}

\setkeys{Gin}{width=0.9\textwidth,height=0.7\textheight}

\title{Statistics}
\subtitle{As I Understand Them}
\author{Isaac Carruthers}

\begin{document}


\frame{\titlepage}


\frame{\frametitle{Distributions: What Are They?}
\center{\emph{A distribution is a mapping from a set of mutually exclusive events to the chance 
of observing each event.}}
\pause

\vspace{12pt}
For instance, when you flip a coin, the distribution of outcomes is
\center{\[\left\{H \rightarrow \frac12, T \rightarrow \frac12\right\}\]}
}


\frame{\frametitle{The Binomial Distribution}
\center{The Binomial Distribution: flip $n$ biased coins (each with probability $p$ of landing
on heads), what is the chance of getting $k$ heads in total?}
\pause

\begin{itemize}
	\item $n=1$ is easy. We flip 1 coin, with chance $p$ of getting one head, and chance $1-p$ of
	getting zero.\pause
	\item $n=2$ has 4 cases: $HH$, $HT$, $TH$, $TT$. Because each flip is independent (one coin
	does not affect the other), the probabilities of these are just $p^2$, $p(1-p)$, $(1-p)p$,
	and $(1-p)^2$ respectively. In turn, the probabilities of getting 2, 1, or 0 heads is $p^2$,
	$2p(1-p)$, and $(1-p)^2$. (Note the 2 ways of getting 1 head).\pause
	\item $n=3$ has 8 cases\ldots which I'm not going to write out.\pause The general formula is
\end{itemize}
\begin{equation*}
	P(K=k;n,p) = \frac{n!}{k!(n-k)!} p^k(1-p)^{n-k}
\end{equation*}
}


\frame{\frametitle{The Binomial Distribution}
\centering
<<binomial,fig=TRUE,echo=FALSE>>=
binplot <- function(n, prob) {
	k <- 0:n
	p <- dbinom(k, n, prob)
	tit <- paste('n = ', n, ', p = ', prob)
	plot(k, p, pch=16, bty='n', las=1, ylim=c(0, max(p)), main=tit, ylab='P(k)')
	segments(k, 0, k, p)
}
par(mfrow=c(2,2))
binplot(2, 0.5)
binplot(5, 0.5)
binplot(10, 0.5)
binplot(20, 0.5)
@
}


\frame{\frametitle{The Poisson Distribution}
\begin{itemize}
	\item The binomial distribution is saying ``I have $n$ opportunities for something to happen,
	how likely is it to happen exactly $k$ times?'' \pause
	\item What if we don't have discrete opportunities, but rather an event that could happen at
	any moment? (Think lightning strikes, for instance). \pause
	\item We can think of this like making $n$ very big, while at the same time makeing $p$ very
	small, so that $np$ remains finite. (e.g. say $np\equiv\lambda$)\pause
\end{itemize}
\begin{equation*}
	\lim_{n\rightarrow\infty} \frac{n!}{k!(n-k)!} p^k(1-p)^{n-k}
	= \frac{e^{-\lambda}\lambda^k}{k!}
\end{equation*}
}


\frame{\frametitle{The Poisson Distribution}
\centering
<<poisson,fig=TRUE,echo=FALSE>>=
poissplot <- function(lambda) {
	mx <- qpois(.999, lambda)
	k <- 0:mx
	p <- dpois(k, lambda)
	tit <- paste0('lambda = ', lambda)
	plot(k, p, pch=16, bty='n', las=1, ylim=c(0, max(p)), main=tit, ylab='P(k)')
	segments(k, 0, k, p)
}
par(mfrow=c(2,2))
poissplot(.1)
poissplot(1)
poissplot(5)
poissplot(50)
@
}


\frame{\frametitle{Continuous Distributions}
\center{What if we don't have discrete events? What if we need to measure the probability of
something continuous, like height, weight, or gender?}\pause
\center{Such variables have ``continuous distributions''.}
}


\frame{\frametitle{Continuous Distributions}
\begin{itemize}
	\item $P(X=x)$ is called the ``probability mass function,'' or pmf of the distribution.\pause
	\item In this case it's no longer useful to talk about $P(X=x)$, as the chance of a continuous
	variable holding exactly a specific value is generally zero.\pause
	\item For continuous distributions, we will work with the ``probability density function''
    (pdf), defined as \[p(x) = \frac{d}{dx} P(X \le x)\]\pause
	\item We can also think of the pdf as
	\[p(x) = \lim_{\epsilon\rightarrow0}\frac1\epsilon P(X\in [x, x+\epsilon])\]
\end{itemize}
}


\frame{\frametitle{Continuous Distributions}
\center{The crucial point is that once we have the pdf of a distribution, we can find the
probability of our random variable $X$ inhabiting any possible set of values $A$.}
\[P(X\in A) = \int_Ap(x) dx\]
}


\frame{\frametitle{Continuous Examples: Uniform Distribution}
\center{The simplest continuous distribution is the uniform distribution, which has a constant
pdf.}
<<uniform,fig=TRUE,echo=FALSE>>=
plot(NA, NA, xlim=c(-.5, 5.5), ylim=c(0, 1), bty='n', las=1, xlab='x', ylab='P(x;a,b)')
unifplot <- function(left, right, col='black') {
	height <- 1 / (right - left)
	segments(c(-.25, 0, right), c(0, height, 0), c(0, right, 5.25), c(0, height, 0), col=col)
	segments(c(0, right), c(0, 0), c(0, right), c(height, height), col=col, lty=2)
	points(c(0, right), c(0, 0), col=col, bg='white', pch=1)
	points(c(0, right), c(height, height), col=col, pch=16)
}
unifplot(0, 1)
unifplot(0, 2, 'darkgreen')
unifplot(0, 5, 'darkred')
@
}


\frame{\frametitle{Continuous Examples: Normal Distribution}
\centering
The most pervasive continuous distribution is the (aptly named) normal distribution:
\begin{equation*}
    p(x;\mu,\sigma) = \frac1{\sqrt{2\sigma^2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
\end{equation*}
<<normal,fig=TRUE,echo=FALSE>>=
x <- seq(-5, 5, length=1000)
p <- exp(-x^2/2)
plot(x, p, ty='l', bty='n', las=1)
@
}


\frame{\frametitle{Fun Facts About the Normal Distribution}\pause
\centering
\begin{itemize}
	\item Everything is a normal distribution (more on this in a moment).\pause
	\item $\sigma$ is called the ``standard deviation'' of the distribution.\pause
	\item There is roughly a 95\% chance of a value being within $2\sigma$ of $\mu$.\pause
	\item This is where we get phrases like ``five sigma detection''.\pause
\end{itemize}
}


\frame{\frametitle{Combining Distributions}
\centering
Suppose we have multiple random variables, and want to know about events that involve all of them.
\pause
\begin{itemize}
	\item We can say that they have a ``joint distribution,'' where each combination of values has
	an associated probability.\pause
	\item The core tool is going to be the ``joint pdf'' (or pmf) $p(x, y)$.\pause
	\item The joint pdf can be defined analagously to the univariate pdf:
	\[p(x,y)=\lim_{\epsilon\rightarrow0}\frac1{\epsilon^2}P(X\in[x,x+\epsilon],Y\in[y,y+\epsilon])\]
	\item It also has the property\pause
	\[P(X,Y\in A) = \iint_Ap(x,y)dxdy\]
	\item The equivalent statement for the pmf is
	\[P(X,Y\in A) = \sum_{x,y\in A} p(x, y)\]
\end{itemize}
}


\frame{\frametitle{Combining Distributions}
\centering
Definition: \emph{Two random variables $x$ and $y$ are \textbf{independent} iff their joint pmf
is the product of their individual pmfs $p(x,y) = p(x)p(y)$}\pause\vspace{12pt}

Consider our coin example. If the two flips ($F_1$ and $F_2$) don't affect each-other, then 
$P(F_1=f_1,F_2=f_2)=P(F_1=f_1)P(F_2=f_2)=\frac14$.\pause\vspace{12pt}

We can use our previous definitions to find the chance of getting exactly one head:
\begin{align*}
	P(F_1,F_2\in \{HT,TH\}) &= \sum_{f_1,f_2\in\{HT,TH\}} \frac14 \\&= \frac12
\end{align*}
}


\frame{\frametitle{Combining Distributions}
\centering
What if we have some function of our variables, and we want to know what the distribution of that
function is going to be?\pause

\begin{align*}
	p(g) \equiv \lim_{\epsilon\rightarrow0}\frac1\epsilon P(G(X,Y) \in [g,g+\epsilon])
    = \iint_{G(x,y)=g} p(x, y) dxdy
\end{align*}\pause

For example, suppose we have $X\sim U(0, 1)$, $Y\sim U(0,1)$, and we want to know the distribution
of $G(X,Y) = X + Y$:
\begin{align*}
	p(g) &= \iint_{x+y=g} 1\,dxdy
\end{align*}
}


\frame{\frametitle{Combining Distributions}
\centering
\begin{align*}
	p(g) &= \iint_{x+y=g} 1\,dxdy
\end{align*}
<<unifsumdomain,fig=TRUE,echo=FALSE>>=
plot(c(0,1), c(1,0), bty='n', las=1, type='l', xlim=c(0,1), ylim=c(0,1), xlab='x', ylab='y');
                                      text(.55, .55, 'x + y = 1', adj=c(0.5, 0), srt=-45)
lines(c(0,.5), c(.5, 0));             text(.3, .3, 'x + y = 0.5', adj=c(0.5, 0), srt=-45)
lines(c(.5,1), c(1, .5));           text(.7, .7, 'x + y = 1.5', adj=c(0.5, 0), srt=-45)
lines(c(0,1,1,0,0), c(0,0,1,1,0), col='gray')
@
}


\frame{\frametitle{Combining Distributions}
\centering
\begin{align*}
	p(g) &= \iint_{x+y=g} 1\,dxdy
\end{align*}
<<unifsum1,fig=TRUE,echo=FALSE>>=
plot(c(0,1,2), c(0,1,0), bty='n', type='l', las=1, xlab='g', ylab='p(g)', ylim=c(0,1))
text(0, 1, 'n = 2', adj=c(0, 1))
@
}


\frame{\frametitle{Combining Distributions}
\centering
\begin{align*}
	p(g) &= \iint_{x+y+z=g} 1\,dxdydz
\end{align*}
<<erwinhall,fig=FALSE,echo=FALSE>>=
erwinhall <- function(x, n) {
	a <- (1 / factorial(n - 1))
	y <- 0 * x
	for (ix in 1:(length(x))) {
		b <- 0
		for (k in 0:floor(x[[ix]])) {
			b <- b + (-1)^k * choose(n, k) * (x[[ix]] - k)^(n - 1)
		}
		y[[ix]] <- a * b
	}
	y
}
@
<<unifsum2,fig=TRUE,echo=FALSE>>=
<<erwinhall>>
x <- seq(0, 3, length=1000)
y <- erwinhall(x, 3)
plot(x, y, bty='n', type='l', las=1, xlab='g', ylab='p(g)', ylim=c(0,1))
text(0, 1, 'n = 3', adj=c(0, 1))
@
}


\frame{\frametitle{Combining Distributions}
\centering
What about with 4 variables?
<<unifsum3,fig=TRUE,echo=FALSE>>=
<<erwinhall>>
x <- seq(0, 4, length=1000)
y <- erwinhall(x, 4)
plot(x, y, bty='n', type='l', las=1, xlab='g', ylab='p(g)', ylim=c(0,1))
text(0, 1, 'n = 4', adj=c(0, 1))
@
}


\frame{\frametitle{Combining Distributions}
\centering
What about with 10 variables?
<<unifsum4,fig=TRUE,echo=FALSE>>=
<<erwinhall>>
x <- seq(0, 10, length=1000)
y <- erwinhall(x, 10)
plot(x, y, bty='n', type='l', las=1, xlab='g', ylab='p(g)', ylim=c(0,1))
text(0, 1, 'n = 10', adj=c(0, 1))
@
}


\frame{\frametitle{Combining Distributions}
\centering
Psych! It's just the normal distribution!
<<unifsumcomp,fig=TRUE,echo=FALSE>>=
<<erwinhall>>
x <- seq(0, 10, length=1000)
y <- erwinhall(x, 10)
z <- dnorm(x, 5, sqrt(10/12.0))
plot(x, y, bty='n', type='l', las=1, xlab='g', ylab='p(g)', ylim=c(0,1))
lines(x, z, col='red', lty=2)
text(0, 1, 'n = 10', adj=c(0, 1))
text(10, 1, 'N(5, sqrt(10/12))', adj=c(1, 1), col='red')
@
}


\frame{\frametitle{Combining Distributions}
\begin{itemize}
	\item Turns out, if you add together enough finite-variance random variables, the sum will
	always have a normal distribution!\pause
	\item This is called the Central Limit Theorem.\pause
	\item This is also why, if we don't know how something is distributed, we generally assume that
	it has a normal distribution.\pause
	\item This can get you into real trouble though, if the underlying variables do not have finite
	variance (see the financial industry), or if you're not adding together enough variables for
	the central limit to be a good estimate.
\end{itemize}
}


\frame{\frametitle{Homework}
\begin{enumerate}
	\item You have a 4-sided die and an 8-sided die, and roll each once. What is the chance that
	the sum of your rolls is 7?
	\item You roll your dice again, but this time you will continue to re-roll them as long as they
	show the same value as each-other. Are your dice independent after your final roll? What is the
	chance they sum to 7? What about 6?
	\item Find the distribution of waiting times between lightning strikes. Assume that strikes are
	independent, and occur with some average rate $\lambda$.
	\textbf{Hint:} start with discrete time-bins,
	find the distribution to waiting times between two time-bins that contain a strike, and then
	take the limit as your bins get small.
\end{enumerate}
}

\end{document}
